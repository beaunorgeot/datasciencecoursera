
#this assumes/depends that you have already run preProcessTitanic!!!!
#this ignores good practices about train, test, validate

library(caret)
library(doMC) #doMC package to take advantage of parallel processing with multiple cores
registerDoMC(cores=4)
library(plyr) #many caret functions use this. Loading plyr after dplyr causes all kinds of problems
library(dplyr)

# CHECK THE PROPORTIONS of survived died.If 1 outcome is far more common than the other you need to change your approach
#This is an important step because if the proportion was smaller than 15%, 
# it would be considered a rare event and would be more challenging to model.
prop.table(table(trainSet$Survived)) #survived is outcome 1 = .38

#Training
myControl <- trainControl(method = "repeatedcv", #use cross-validation
                          number = 10, repeats = 5 )

# see what models are available
names(getModelInfo())

#trees like the x,y input: the other methods included here require! the formula input
# Actually: models generated from caret::train() that contain factors, if the model is presented as a formula (y ~ x1 + x2 + x3) and not as y = y, x = x (as a matrix or data.frame). Only through the formula interface will the the model create the proper contrasts or dummy variables. 
#modRF <- train(y=trainSet$Survived, x=trainSet, method = "rf", trControl = myControl)
modRF <- train(Survived ~., data = trainSet, method = "rf", trControl = myControl)
#modGBM <- train(y=trainSet$Survived, x=trainSet, method = "gbm", trControl = myControl, verbose = F) 
modGBM <- train(Survived ~., data = trainSet, method = "gbm", trControl = myControl, verbose = F) 
modLDA <- train(Survived ~., data = trainSet,method = "lda", preProcess=c("center","scale"), trControl = myControl) 
modGLMNET <- train(Survived ~., data = trainSet, method = "glmnet", preProcess=c("center","scale"), trControl = myControl) 
modSVM <- train(Survived ~., data = trainSet, method = "svmRadial", preProcess=c("center","scale"), trControl = myControl) 

#Here is the caret method for checking the correlation of the models. CURRENTLY FAILING for LDA
modCor <- modelCor(resamples(list(RF = modRF, GBM = modGBM, LDA = modLDA, GLMNET = modGLMNET, SVM = modSVM))) 
# Pearson's r Correlation: (-1,0,1) +/-1 = perfect correlation, 0 = no correlation (-.2,.2) is what we want. Up to +/-.3 might be acceptable
# we see no correlation between models

#generate predictions. In a good workflow(train,test,validate), this would be done on the testSet
predRF <- predict(modRF,trainSet)
predGBM <- predict(modGBM, trainSet)
predLDA <- predict(modLDA, trainSet)
predGLMNET <- predict(modGLMNET, trainSet)
predSVM <- predict(modSVM, trainSet)

# We've already checked to make sure that the models aren't highly correlated
# We can also check to see if predictions generated by the models are correlated. I'm not sure how useful this is yet
predDF1 <- predDF
predDF1$predRF <- as.numeric(predDF1$predRF)
predDF1$predGBM <- as.numeric(predDF1$predGBM)
predDF1$predLDA <- as.numeric(predDF1$predLDA)
predDF1$Survived <- as.numeric(predDF1$Survived)
cor(predDF1) # this works, and it checks the correlation of the predictions LDA and GBM are highly correlated, the others are not 

#combine predictions, fit a meta-model
predDF <- data.frame(predRF,predGBM,predLDA,predGLMNET,predSVM,Survived=trainSet$Survived)
combFit <- train(Survived ~.,method="rf",data=predDF, trControl = myControl)
predComb <- predict(combFit,trainSet)
combFitGB <- train(Survived ~.,method="gbm",data=predDF, trControl = myControl)
predCombGB <- predict(combFitGB, trainSet)

# Initial accuracy check
# Get/compare the accuracies for the 3 singular models and the 1 combined model (4 models)
c1 <- confusionMatrix(predRF, trainSet$Survived)$overall[1]
c2 <- confusionMatrix(predGBM, trainSet$Survived)$overall[1]
c3 <- confusionMatrix(predLDA, trainSet$Survived)$overall[1]
c4 <- confusionMatrix(predGLMNET, trainSet$Survived)$overall[1]
c5 <- confusionMatrix(predSVM, trainSet$Survived)$overall[1]
c6 <- confusionMatrix(predComb, trainSet$Survived)$overall[1]
c7 <- confusionMatrix(predCombGB, trainSet$Survived)$overall[1]
print(paste(c1, c2, c3, c4,c5,c6,c7)) #0.958473625140292 0.877665544332211 0.84287317620651 0.838383838383838 0.854096520763187 0.958473625140292" 
# Not surprising, training and predicting on same data here, won't see this in normal work flow
# worth noting that combFit outcome was identical to fitRF, Is combFit just using fitRF? YES using gbm we can see that predRF is given 99.95% of the vote. Maybe a 
# greedy ensemble is the way to go?


#Now repeat on testSet making sure same names are used as input
predRF <- predict(modRF,testSet)
predGBM <- predict(modGBM,testSet)
predLDA <- predict(modLDA,testSet)
predGLMNET <- predict(modGLMNET,testSet)
predSVM <- predict(modSVM,testSet)

predDF <- data.frame(predRF,predGBM,predLDA,predGLMNET,predSVM)
#Generate final predictions
testSet$Survived <- predict(combFit, predDF)

#Generate SUBMISSION
stackedSubmission <- testSet %>% select(PassengerId,Survived) #stackedSubmission <- subset(testSet, select= c(PassengerId,Survived))
# kaggle requires that the outcome is just (0,1)
stackedSubmission1 <- stackedSubmission %>% mutate(Survived = ifelse(Survived == "Survived", 1,0)) 
stackedSubmission %>% group_by(Survived) %>% summarise(n()) #Died 269, Survived 149

#write resulting predictions w/only the two columns to csv
write.table(stackedSubmission1,file = "stackedSubmission.csv", col.names = TRUE, row.names = FALSE, sep = ",")